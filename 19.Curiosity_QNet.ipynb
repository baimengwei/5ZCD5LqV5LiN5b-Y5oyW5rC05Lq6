{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"19.Curiosity_QNet.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyN4wHQLNE2MgtBGWB5mbzpg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"J82GGgUOSgk_"},"source":["代码实现效果不佳，不如TensorFlow实现结果稳定。当超参数一致时，Torch实现收敛效果。\n","\n","该代码只适合于MountainCar环境，在CartPole环境中无法收敛。\n","\n","思想是提供了一个D(s,a)->s'，s'-s_t得到的均方差作为奖励的一部分，同时对该D网络进行训练最小化，当s与a的组合对未出现过，则值更大，鼓励探索"]},{"cell_type":"markdown","metadata":{"id":"C7TJsNT4e8Mo"},"source":["运行收敛需要多次运行得到想要的结果，否则使用TensorFlow？"]},{"cell_type":"markdown","metadata":{"id":"PHKUXmTzTrxs"},"source":["# 导入函数包"]},{"cell_type":"code","metadata":{"id":"T5M_ZoNjSbWI","executionInfo":{"status":"ok","timestamp":1622556094744,"user_tz":-480,"elapsed":1565,"user":{"displayName":"白梦伟","photoUrl":"","userId":"14318251260292756104"}}},"source":["import numpy as np\n","import torch\n","from torch import nn\n","import gym\n","torch.set_default_dtype(torch.float64)"],"execution_count":211,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4QvGYjoTn-O"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aJHXNxxkTv-A"},"source":["# 经验池"]},{"cell_type":"code","metadata":{"id":"nhl5OqdRTn7w"},"source":["class memory:\n","    def __init__(self, env, memory_length=10000, memory_minibatch=128):\n","        self.env = env\n","        self.memory_length = memory_length\n","        self.memory_minibatch = memory_minibatch\n","\n","        self.state_size = self.env.observation_space.shape[0]\n","        try:\n","            self.action_size = self.env.action_space.shape[0]\n","        except:\n","            self.action_size = 1\n","\n","        self.memory_width = self.state_size * 2 + self.action_size + 1\n","        self.memory = np.zeros((self.memory_length, self.memory_width))\n","\n","        self.index = 0\n","        self.max_index = 0\n","\n","    def store(self, state, action, reward, next_state):\n","        transacton = np.hstack((state, action, reward, next_state))\n","        self.memory[self.index, :] = transacton\n","\n","        self.index += 1\n","        if self.index % self.memory_length == 0:\n","            self.index = 0\n","        if self.max_index < self.memory_length:\n","            self.max_index += 1\n","\n","    def sample(self):\n","        choice_random = np.random.choice(self.max_index, self.memory_minibatch)\n","        choice_data = self.memory[choice_random, :]\n","        state = choice_data[:, 0:self.state_size]\n","        action = choice_data[:,\n","                             self.state_size:self.state_size + self.action_size]\n","        reward = choice_data[:, self.state_size + self.action_size:\n","                             self.state_size + self.action_size + 1]\n","        next_state = choice_data[:, self.state_size + self.action_size + 1:]\n","\n","        reward = np.squeeze(reward)\n","        action = np.squeeze(action)\n","        \n","        return state, action, reward, next_state\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xE4Bv_vITn5Q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DJaWBybAT04w"},"source":["# 好奇心网络"]},{"cell_type":"code","metadata":{"id":"2FJ7wcIATn2n"},"source":["class curiosity_net(nn.Module):\n","    def __init__(self, state_dim, hidden_dim=32):\n","        super().__init__()\n","        self.state_dim = state_dim\n","        self.action_dim = 1\n","        self.hidden_dim = hidden_dim\n","\n","        self.linear1 = nn.Linear(self.state_dim, self.hidden_dim)\n","        self.linear2 = nn.Linear(self.action_dim, self.hidden_dim)\n","        self.activate = nn.ReLU()\n","        self.linear3 = nn.Linear(self.hidden_dim, self.state_dim)\n","\n","    def forward(self, s, a):\n","        x1 = self.linear1(s)\n","        x2 = self.linear2(a)\n","        x = x1 + x2\n","        x = self.activate(x)\n","        x = self.linear3(x)\n","        return x\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gwKq1jffTnz-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1W_wa6tiT5Y-"},"source":["# Q网络"]},{"cell_type":"code","metadata":{"id":"sysOJ6IzTnxO"},"source":["class network:\n","    def __init__(self, env, hidden_dimension=128, learning_rate=1e-3):\n","        self.env = env\n","        self.hidden_dimension = hidden_dimension\n","        self.learning_rate = learning_rate\n","\n","        self.input_dimension = self.env.observation_space.shape[0]\n","        self.output_dimension = self.env.action_space.n\n","\n","        self.model = self.__create_network()\n","        self.model_curiosity = curiosity_net(self.input_dimension,\n","                            self.output_dimension)\n","\n","        self.loss = torch.nn.MSELoss()\n","        self.optimizer = torch.optim.RMSprop(self.model.parameters(),\n","                            lr=self.learning_rate)\n","        self.optimizer_curiosity = torch.optim.RMSprop(self.model_curiosity.parameters(),\n","                                lr=0.01)\n","\n","    @staticmethod\n","    def replace(network_from, network_to):\n","        network_to.load_state_dict(network_from.state_dict())\n","\n","    @staticmethod\n","    def optimizer(predict_object, predict_value, target_value):\n","        loss = predict_object.loss(predict_value, target_value)\n","        predict_object.optimizer.zero_grad()\n","        loss.backward()\n","        predict_object.optimizer.step()\n","        return loss.item()\n","\n","\n","    class q_network_model(nn.Module):\n","        def __init__(self, in_dim, out_dim, hidden_dim):\n","            super().__init__()\n","            self.in_dim = in_dim\n","            self.out_dim = out_dim\n","            self.hidden_dim = hidden_dim\n","\n","            self.linear1 = nn.Linear(self.in_dim, self.hidden_dim)\n","            self.linear2 = nn.Linear(self.hidden_dim, self.out_dim)\n","            self.activate = nn.ReLU()\n","\n","        def forward(self, s):\n","            x = self.linear1(s)\n","            x = self.activate(x)\n","            x = self.linear2(x)\n","            return x\n","\n","    def __create_network(self):\n","        return self.q_network_model(self.input_dimension,\n","                      self.output_dimension,\n","                      self.hidden_dimension)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16t8h-HUTnun"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4RqCTrm2T_F-"},"source":["# 智能体"]},{"cell_type":"code","metadata":{"id":"EUY9azTfTnsO"},"source":["class agent_q:\n","    def __init__(self, env, gamma=0.9):\n","        self.env = env\n","        self.gamma = gamma\n","\n","        self.epislon_method = self.epislon_method_1()\n","\n","        self.q_network = network(self.env)\n","        self.q_network_target = network(self.env)\n","        self.memory = memory(self.env)\n","\n","        self.epislon_learn_step = 0\n","        pass\n","\n","    def output_action(self, state):\n","        state = torch.from_numpy(state)\n","        action_value = self.q_network.model(state)\n","        action_value = np.array(action_value.tolist())\n","\n","        random_number = np.random.random()\n","        if random_number > self.epislon_method.epislon_init:\n","            action = self.env.action_space.sample()\n","        else:\n","            action = np.argmax(action_value)\n","            action = np.squeeze(action)\n","        return action, action_value\n","\n","    def sample_postprocess(self):\n","        state, action, reward, next_state = self.memory.sample()\n","        state = torch.from_numpy(state)\n","        next_state = torch.from_numpy(next_state)\n","        action = np.squeeze(action)\n","        reward = np.squeeze(reward)\n","        action = action.astype(np.int32)\n","        return state, action, reward, next_state\n","\n","    def learn(self):\n","        self.epislon_learn_step += 1\n","\n","        if self.epislon_learn_step % 300 == 0:\n","            network.replace(self.q_network.model,\n","                    self.q_network_target.model)\n","            \n","        state, action, reward, next_state = self.sample_postprocess()\n","        next_state_fit = self.q_network.model_curiosity(\n","            state, torch.Tensor(action[np.newaxis].T))\n","        loss = nn.functional.mse_loss(next_state_fit, next_state)\n","        loss_value = torch.sum(torch.pow(next_state_fit - next_state, 2), axis=1).detach().numpy()\n","\n","        if self.epislon_learn_step % 1000 == 0:\n","            self.q_network.optimizer_curiosity.zero_grad()\n","            loss.backward()\n","            # print('loss item:', loss.item())\n","            self.q_network.optimizer_curiosity.step()\n","\n","        reward += loss_value\n","        # print(reward)\n","\n","        target_value_max, target_action_max = torch.max(\n","            self.q_network_target.model(next_state), axis=1)\n","        target_value = reward + self.gamma * \\\n","            np.array(target_value_max.tolist())\n","\n","        predict_value_all = self.q_network.model(state)\n","\n","        replace_index = np.arange(self.memory.memory_minibatch, dtype=np.int32)\n","        target_value_all = np.array(predict_value_all.tolist())\n","        target_value_all[replace_index, action] = target_value\n","\n","        self.epislon_method.update()\n","\n","        return network.optimizer(self.q_network,\n","                    predict_value_all,\n","                    torch.from_numpy(target_value_all))\n","\n","    class epislon_method_1:\n","        def __init__(self):\n","            self.epislon_init = 0.01\n","            self.epislon_increment = 1.001\n","            self.epislon_max = 0.90\n","\n","        def update(self):\n","            if self.epislon_init < self.epislon_max:\n","                self.epislon_init *= self.epislon_increment\n","\n","    class epislon_method_2:\n","        def __init__(self):\n","            self.epislon_init = 0.95\n","\n","        def update(self):\n","            pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iNttPNVtTnpe"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uaRHu2X1UC2g"},"source":["# 交互部分"]},{"cell_type":"code","metadata":{"id":"PUV7wxvOTnm-"},"source":["class interactive:\n","    def __init__(self, env, epoch_max=1000, epoch_replace=1):\n","        self.env = env\n","        self.epoch_max = epoch_max\n","        self.epoch_replace = epoch_replace\n","\n","        self.env = self.env.unwrapped\n","        self.agent = agent_q(self.env)\n","\n","    def start_execute(self):\n","        self.epoch_index = 0\n","        self.loss_value = 0\n","        for i in range(self.epoch_max):\n","            self.epoch_index += 1\n","            state = self.env.reset()\n","            self.epoch_step = 0\n","            while True:\n","                self.epoch_step += 1\n","                action, _ = self.agent.output_action(state)\n","                next_state, reward, done, info = self.env.step(action)\n","\n","                self.agent.memory.store(state, action, reward, next_state)\n","                state = next_state\n","\n","                self.loss_value = self.agent.learn()\n","\n","                if done:\n","                    break\n","            self.statistic()\n","\n","    def statistic(self):\n","        if not self.epoch_index > 1:\n","            self.epoch_step_list = []\n","            self.loss_value_list = []\n","        print('epoch %-5s, length %-5s, loss_value %5f, epislon %5f' %\n","                (self.epoch_index, self.epoch_step, self.loss_value,\n","                self.agent.epislon_method.epislon_init))\n","        self.epoch_step_list.append(self.epoch_step)\n","        self.loss_value_list.append(self.loss_value)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zpXIZx2iTnke"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9zn7Xf84UFbC"},"source":["# 代码执行"]},{"cell_type":"code","metadata":{"id":"2hOCwSarTnh3"},"source":["if __name__ == '__main__':\n","    env_name = 'MountainCar-v0'\n","    env = gym.make(env_name)\n","    dqn_evoluate = interactive(env, epoch_max=50)\n","    dqn_evoluate.start_execute()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SwWgIzseUKQi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HeFCNVAJUKNw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x3VJtzxazohu"},"source":["# 随机网络蒸馏"]},{"cell_type":"markdown","metadata":{"id":"aeoVCLf7zxSm"},"source":["## 编码网络"]},{"cell_type":"code","metadata":{"id":"hSRP8dDozwx2","executionInfo":{"status":"ok","timestamp":1622556099976,"user_tz":-480,"elapsed":423,"user":{"displayName":"白梦伟","photoUrl":"","userId":"14318251260292756104"}}},"source":["class encoder_net(nn.Module):\n","    def __init__(self, state_dim, encoder_dim=1000):\n","        super().__init__()\n","        self.linear1 = nn.Linear(state_dim, encoder_dim)\n","\n","    def forward(self, s):\n","        x = self.linear1(s)\n","        return x"],"execution_count":212,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6N6zyUYN0MkB"},"source":["## 预训练网络"]},{"cell_type":"code","metadata":{"id":"zDqT6c1EzoUz","executionInfo":{"status":"ok","timestamp":1622556102561,"user_tz":-480,"elapsed":1,"user":{"displayName":"白梦伟","photoUrl":"","userId":"14318251260292756104"}}},"source":["class pre_train_net(nn.Module):\n","    def __init__(self, state_dim, hidden_dim=128, encoder_dim=1000):\n","        super().__init__()\n","        self.linear1 = nn.Linear(state_dim, hidden_dim)\n","        self.activate = nn.ReLU()\n","        self.linear2 = nn.Linear(hidden_dim, encoder_dim)\n","\n","    def forward(self, s):\n","        x = self.activate(self.linear1(s))\n","        x = self.linear2(x)\n","        return x"],"execution_count":213,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p5zAvbMJ2RWu"},"source":["## Q网络"]},{"cell_type":"code","metadata":{"id":"QYuoy_uQzoSS","executionInfo":{"status":"ok","timestamp":1622556149400,"user_tz":-480,"elapsed":476,"user":{"displayName":"白梦伟","photoUrl":"","userId":"14318251260292756104"}}},"source":["class network:\n","    def __init__(self, env, hidden_dimension=128, learning_rate=1e-3):\n","        self.env = env\n","        self.hidden_dimension = hidden_dimension\n","        self.learning_rate = learning_rate\n","\n","        self.input_dimension = self.env.observation_space.shape[0]\n","        self.output_dimension = self.env.action_space.n\n","\n","        self.model = self.__create_network()\n","        self.encoder_net = encoder_net(self.input_dimension)\n","        self.model_pre_train = pre_train_net(self.input_dimension)\n","\n","        self.loss = torch.nn.MSELoss()\n","        self.optimizer = torch.optim.RMSprop(self.model.parameters(),\n","                            lr=self.learning_rate)\n","        self.optimizer_pre_train = torch.optim.RMSprop(self.model_pre_train.parameters(),\n","                                lr=0.01)\n","\n","    @staticmethod\n","    def replace(network_from, network_to):\n","        network_to.load_state_dict(network_from.state_dict())\n","\n","    @staticmethod\n","    def optimizer(predict_object, predict_value, target_value):\n","        loss = predict_object.loss(predict_value, target_value)\n","        predict_object.optimizer.zero_grad()\n","        loss.backward()\n","        predict_object.optimizer.step()\n","        return loss.item()\n","\n","\n","    class q_network_model(nn.Module):\n","        def __init__(self, in_dim, out_dim, hidden_dim):\n","            super().__init__()\n","            self.in_dim = in_dim\n","            self.out_dim = out_dim\n","            self.hidden_dim = hidden_dim\n","\n","            self.linear1 = nn.Linear(self.in_dim, self.hidden_dim)\n","            self.linear2 = nn.Linear(self.hidden_dim, self.out_dim)\n","            self.activate = nn.ReLU()\n","\n","        def forward(self, s):\n","            x = self.linear1(s)\n","            x = self.activate(x)\n","            x = self.linear2(x)\n","            return x\n","\n","    def __create_network(self):\n","        return self.q_network_model(self.input_dimension,\n","                      self.output_dimension,\n","                      self.hidden_dimension)\n","\n"],"execution_count":218,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_0B9bldp2-z5"},"source":["## 智能体"]},{"cell_type":"code","metadata":{"id":"PUz0vFl22-aA","executionInfo":{"status":"ok","timestamp":1622556176351,"user_tz":-480,"elapsed":2,"user":{"displayName":"白梦伟","photoUrl":"","userId":"14318251260292756104"}}},"source":["class agent_q:\n","    def __init__(self, env, gamma=0.9):\n","        self.env = env\n","        self.gamma = gamma\n","\n","        self.epislon_method = self.epislon_method_1()\n","\n","        self.q_network = network(self.env)\n","        self.q_network_target = network(self.env)\n","        self.memory = memory(self.env)\n","\n","        self.epislon_learn_step = 0\n","        pass\n","\n","    def output_action(self, state):\n","        state = torch.from_numpy(state)\n","        action_value = self.q_network.model(state)\n","        action_value = np.array(action_value.tolist())\n","\n","        random_number = np.random.random()\n","        if random_number > self.epislon_method.epislon_init:\n","            action = self.env.action_space.sample()\n","        else:\n","            action = np.argmax(action_value)\n","            action = np.squeeze(action)\n","        return action, action_value\n","\n","    def sample_postprocess(self):\n","        state, action, reward, next_state = self.memory.sample()\n","        state = torch.from_numpy(state)\n","        next_state = torch.from_numpy(next_state)\n","        action = np.squeeze(action)\n","        reward = np.squeeze(reward)\n","        action = action.astype(np.int32)\n","        return state, action, reward, next_state\n","\n","    def learn(self):\n","        self.epislon_learn_step += 1\n","\n","        if self.epislon_learn_step % 300 == 0:\n","            network.replace(self.q_network.model,\n","                    self.q_network_target.model)\n","            \n","        state, action, reward, next_state = self.sample_postprocess()\n","        next_state_encoder = self.q_network.encoder_net(next_state)\n","        next_state_pre_train = self.q_network.model_pre_train(next_state)\n","\n","        loss = nn.functional.mse_loss(next_state_pre_train, next_state_encoder)\n","        loss_value = torch.sum(torch.pow(next_state_pre_train - next_state_encoder, 2),\n","                               axis=1).detach().numpy()\n","\n","        if self.epislon_learn_step % 1000 == 0:\n","            self.q_network.optimizer_pre_train.zero_grad()\n","            loss.backward()\n","            # print('loss item:', loss.item())\n","            self.q_network.optimizer_pre_train.step()\n","\n","        reward += loss_value\n","        # print(reward)\n","\n","        target_value_max, target_action_max = torch.max(\n","            self.q_network_target.model(next_state), axis=1)\n","        target_value = reward + self.gamma * \\\n","            np.array(target_value_max.tolist())\n","\n","        predict_value_all = self.q_network.model(state)\n","\n","        replace_index = np.arange(self.memory.memory_minibatch, dtype=np.int32)\n","        target_value_all = np.array(predict_value_all.tolist())\n","        target_value_all[replace_index, action] = target_value\n","\n","        self.epislon_method.update()\n","\n","        return network.optimizer(self.q_network,\n","                    predict_value_all,\n","                    torch.from_numpy(target_value_all))\n","\n","    class epislon_method_1:\n","        def __init__(self):\n","            self.epislon_init = 0.01\n","            self.epislon_increment = 1.001\n","            self.epislon_max = 0.90\n","\n","        def update(self):\n","            if self.epislon_init < self.epislon_max:\n","                self.epislon_init *= self.epislon_increment\n","\n","    class epislon_method_2:\n","        def __init__(self):\n","            self.epislon_init = 0.95\n","\n","        def update(self):\n","            pass\n"],"execution_count":220,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ctPWKg9Q4LIW"},"source":["## 交互部分"]},{"cell_type":"code","metadata":{"id":"lrNis5O-zoP_","executionInfo":{"status":"ok","timestamp":1622556114708,"user_tz":-480,"elapsed":2101,"user":{"displayName":"白梦伟","photoUrl":"","userId":"14318251260292756104"}}},"source":["class interactive:\n","    def __init__(self, env, epoch_max=1000, epoch_replace=1):\n","        self.env = env\n","        self.epoch_max = epoch_max\n","        self.epoch_replace = epoch_replace\n","\n","        self.env = self.env.unwrapped\n","        self.agent = agent_q(self.env)\n","\n","    def start_execute(self):\n","        self.epoch_index = 0\n","        self.loss_value = 0\n","        for i in range(self.epoch_max):\n","            self.epoch_index += 1\n","            state = self.env.reset()\n","            self.epoch_step = 0\n","            while True:\n","                self.epoch_step += 1\n","                action, _ = self.agent.output_action(state)\n","                next_state, reward, done, info = self.env.step(action)\n","\n","                self.agent.memory.store(state, action, reward, next_state)\n","                state = next_state\n","\n","                self.loss_value = self.agent.learn()\n","\n","                if done:\n","                    break\n","            self.statistic()\n","\n","    def statistic(self):\n","        if not self.epoch_index > 1:\n","            self.epoch_step_list = []\n","            self.loss_value_list = []\n","        print('epoch %-5s, length %-5s, loss_value %5f, epislon %5f' %\n","                (self.epoch_index, self.epoch_step, self.loss_value,\n","                self.agent.epislon_method.epislon_init))\n","        self.epoch_step_list.append(self.epoch_step)\n","        self.loss_value_list.append(self.loss_value)\n","\n"],"execution_count":216,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U8sWPb6o4O6A"},"source":["## 代码执行"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DSqp4tQlzoNO","executionInfo":{"status":"ok","timestamp":1622556282676,"user_tz":-480,"elapsed":102123,"user":{"displayName":"白梦伟","photoUrl":"","userId":"14318251260292756104"}},"outputId":"9c2bcaf7-1978-490c-ae0f-41af65f564cd"},"source":["if __name__ == '__main__':\n","    env_name = 'MountainCar-v0'\n","    env = gym.make(env_name)\n","    dqn_evoluate = interactive(env, epoch_max=50)\n","    dqn_evoluate.start_execute()"],"execution_count":221,"outputs":[{"output_type":"stream","text":["epoch 1    , length 6990 , loss_value 8.370674, epislon 0.900847\n","epoch 2    , length 8051 , loss_value 0.130469, epislon 0.900847\n","epoch 3    , length 477  , loss_value 0.123358, epislon 0.900847\n","epoch 4    , length 318  , loss_value 0.357393, epislon 0.900847\n","epoch 5    , length 416  , loss_value 0.086545, epislon 0.900847\n","epoch 6    , length 2489 , loss_value 0.117268, epislon 0.900847\n","epoch 7    , length 178  , loss_value 0.047599, epislon 0.900847\n","epoch 8    , length 180  , loss_value 0.039014, epislon 0.900847\n","epoch 9    , length 327  , loss_value 0.046421, epislon 0.900847\n","epoch 10   , length 183  , loss_value 0.057766, epislon 0.900847\n","epoch 11   , length 180  , loss_value 0.024246, epislon 0.900847\n","epoch 12   , length 334  , loss_value 0.080988, epislon 0.900847\n","epoch 13   , length 193  , loss_value 0.026834, epislon 0.900847\n","epoch 14   , length 199  , loss_value 0.040778, epislon 0.900847\n","epoch 15   , length 167  , loss_value 0.046859, epislon 0.900847\n","epoch 16   , length 147  , loss_value 0.050205, epislon 0.900847\n","epoch 17   , length 216  , loss_value 0.042813, epislon 0.900847\n","epoch 18   , length 188  , loss_value 0.100392, epislon 0.900847\n","epoch 19   , length 181  , loss_value 0.074685, epislon 0.900847\n","epoch 20   , length 720  , loss_value 0.028355, epislon 0.900847\n","epoch 21   , length 143  , loss_value 0.043860, epislon 0.900847\n","epoch 22   , length 132  , loss_value 0.033619, epislon 0.900847\n","epoch 23   , length 176  , loss_value 0.055839, epislon 0.900847\n","epoch 24   , length 197  , loss_value 0.052493, epislon 0.900847\n","epoch 25   , length 549  , loss_value 0.075757, epislon 0.900847\n","epoch 26   , length 689  , loss_value 1.099274, epislon 0.900847\n","epoch 27   , length 231  , loss_value 0.103137, epislon 0.900847\n","epoch 28   , length 282  , loss_value 0.451189, epislon 0.900847\n","epoch 29   , length 273  , loss_value 0.234926, epislon 0.900847\n","epoch 30   , length 244  , loss_value 0.195994, epislon 0.900847\n","epoch 31   , length 327  , loss_value 0.410973, epislon 0.900847\n","epoch 32   , length 164  , loss_value 1.189278, epislon 0.900847\n","epoch 33   , length 289  , loss_value 0.193047, epislon 0.900847\n","epoch 34   , length 160  , loss_value 0.128932, epislon 0.900847\n","epoch 35   , length 324  , loss_value 0.383575, epislon 0.900847\n","epoch 36   , length 202  , loss_value 0.099958, epislon 0.900847\n","epoch 37   , length 153  , loss_value 1.036462, epislon 0.900847\n","epoch 38   , length 174  , loss_value 0.076419, epislon 0.900847\n","epoch 39   , length 154  , loss_value 0.076583, epislon 0.900847\n","epoch 40   , length 154  , loss_value 0.136559, epislon 0.900847\n","epoch 41   , length 122  , loss_value 0.105179, epislon 0.900847\n","epoch 42   , length 176  , loss_value 0.179257, epislon 0.900847\n","epoch 43   , length 126  , loss_value 0.071318, epislon 0.900847\n","epoch 44   , length 175  , loss_value 0.030803, epislon 0.900847\n","epoch 45   , length 399  , loss_value 0.020026, epislon 0.900847\n","epoch 46   , length 182  , loss_value 0.044602, epislon 0.900847\n","epoch 47   , length 674  , loss_value 0.055620, epislon 0.900847\n","epoch 48   , length 242  , loss_value 0.052477, epislon 0.900847\n","epoch 49   , length 125  , loss_value 0.067412, epislon 0.900847\n","epoch 50   , length 171  , loss_value 0.039648, epislon 0.900847\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rICYl3La4Q7n"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Mrm-DaI4Qzh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x3oj6dWe4Qw1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MvVk2rXVVIYT"},"source":["# 工具参考"]},{"cell_type":"markdown","metadata":{"id":"dR06wjbvUZwP"},"source":["## TensorFlow实现"]},{"cell_type":"code","metadata":{"id":"riiZXWpkUKLC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ehFnZzPhUKIf"},"source":["import numpy as np\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior() \n","import gym\n","import matplotlib.pyplot as plt\n","from gym import wrappers\n","tf.reset_default_graph()\n","\n","class CuriosityNet:\n","    def __init__(\n","            self,\n","            n_a,\n","            n_s,\n","            lr=0.01,\n","            gamma=0.98,\n","            epsilon=0.95,\n","            replace_target_iter=300,\n","            memory_size=10000,\n","            batch_size=128,\n","            output_graph=False,\n","    ):\n","        self.n_a = n_a\n","        self.n_s = n_s\n","        self.lr = lr\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.replace_target_iter = replace_target_iter\n","        self.memory_size = memory_size\n","        self.batch_size = batch_size\n","\n","        # total learning step\n","        self.learn_step_counter = 0\n","        self.memory_counter = 0\n","\n","        # initialize zero memory [s, a, r, s_]\n","        self.memory = np.zeros((self.memory_size, n_s * 2 + 2))\n","        self.tfs, self.tfa, self.tfr, self.tfs_, self.dyn_train, self.dqn_train, self.q, self.int_r = \\\n","            self._build_nets()\n","\n","        t_params = tf.get_collection(\n","            tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')\n","        e_params = tf.get_collection(\n","            tf.GraphKeys.GLOBAL_VARIABLES, scope='eval_net')\n","\n","        with tf.variable_scope('hard_replacement'):\n","            self.target_replace_op = [\n","                tf.assign(t, e) for t, e in zip(t_params, e_params)]\n","\n","        self.sess = tf.Session()\n","\n","        if output_graph:\n","            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n","\n","        self.sess.run(tf.global_variables_initializer())\n","\n","    def _build_nets(self):\n","        tfs = tf.placeholder(\n","            tf.float32, [None, self.n_s], name=\"s\")    # input State\n","        # input Action\n","        tfa = tf.placeholder(tf.int32, [None, ], name=\"a\")\n","        # extrinsic reward\n","        tfr = tf.placeholder(tf.float32, [None, ], name=\"ext_r\")\n","        tfs_ = tf.placeholder(\n","            tf.float32, [None, self.n_s], name=\"s_\")  # input Next State\n","\n","        # dynamics net\n","        dyn_s_, curiosity, dyn_train = self._build_dynamics_net(tfs, tfa, tfs_)\n","\n","        # normal RL model\n","        total_reward = tf.add(curiosity, tfr, name=\"total_r\")\n","        q, dqn_loss, dqn_train = self._build_dqn(tfs, tfa, total_reward, tfs_)\n","        return tfs, tfa, tfr, tfs_, dyn_train, dqn_train, q, curiosity\n","\n","    def _build_dynamics_net(self, s, a, s_):\n","        with tf.variable_scope(\"dyn_net\"):\n","            float_a = tf.expand_dims(\n","                tf.cast(a, dtype=tf.float32, name=\"float_a\"), axis=1, name=\"2d_a\")\n","            sa = tf.concat((s, float_a), axis=1, name=\"sa\")\n","            encoded_s_ = s_                # here we use s_ as the encoded s_\n","\n","            dyn_l = tf.layers.dense(sa, 32, activation=tf.nn.relu)\n","            dyn_s_ = tf.layers.dense(dyn_l, self.n_s)  # predicted s_\n","        with tf.name_scope(\"int_r\"):\n","            squared_diff = tf.reduce_sum(\n","                tf.square(encoded_s_ - dyn_s_), axis=1)  # intrinsic reward\n","\n","        # It is better to reduce the learning rate in order to stay curious\n","        train_op = tf.train.RMSPropOptimizer(\n","            self.lr, name=\"dyn_opt\").minimize(tf.reduce_mean(squared_diff))\n","        return dyn_s_, squared_diff, train_op\n","\n","    def _build_dqn(self, s, a, r, s_):\n","        with tf.variable_scope('eval_net'):\n","            e1 = tf.layers.dense(s, 128, tf.nn.relu)\n","            q = tf.layers.dense(e1, self.n_a, name=\"q\")\n","        with tf.variable_scope('target_net'):\n","            t1 = tf.layers.dense(s_, 128, tf.nn.relu)\n","            q_ = tf.layers.dense(t1, self.n_a, name=\"q_\")\n","\n","        with tf.variable_scope('q_target'):\n","            q_target = r + self.gamma * \\\n","                tf.reduce_max(q_, axis=1, name=\"Qmax_s_\")\n","\n","        with tf.variable_scope('q_wrt_a'):\n","            a_indices = tf.stack(\n","                [tf.range(tf.shape(a)[0], dtype=tf.int32), a], axis=1)\n","            q_wrt_a = tf.gather_nd(params=q, indices=a_indices)\n","\n","        loss = tf.losses.mean_squared_error(\n","            labels=q_target, predictions=q_wrt_a)   # TD error\n","        train_op = tf.train.RMSPropOptimizer(self.lr, name=\"dqn_opt\").minimize(\n","            loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"eval_net\"))\n","        return q, loss, train_op\n","\n","    def store_transition(self, s, a, r, s_):\n","        transition = np.hstack((s, [a, r], s_))\n","        # replace the old memory with new memory\n","        index = self.memory_counter % self.memory_size\n","        self.memory[index, :] = transition\n","        self.memory_counter += 1\n","\n","    def choose_action(self, observation):\n","        # to have batch dimension when feed into tf placeholder\n","        s = observation[np.newaxis, :]\n","\n","        if np.random.uniform() < self.epsilon:\n","            # forward feed the observation and get q value for every actions\n","            actions_value = self.sess.run(self.q, feed_dict={self.tfs: s})\n","            action = np.argmax(actions_value)\n","        else:\n","            action = np.random.randint(0, self.n_a)\n","        return action\n","\n","    def learn(self):\n","        # check to replace target parameters\n","        if self.learn_step_counter % self.replace_target_iter == 0:\n","            self.sess.run(self.target_replace_op)\n","\n","        # sample batch memory from all memory\n","        top = self.memory_size if self.memory_counter > self.memory_size else self.memory_counter\n","        sample_index = np.random.choice(top, size=self.batch_size)\n","        batch_memory = self.memory[sample_index, :]\n","\n","        bs, ba, br, bs_ = batch_memory[:, :self.n_s], batch_memory[:, self.n_s], \\\n","            batch_memory[:, self.n_s + 1], batch_memory[:, -self.n_s:]\n","        self.sess.run(self.dqn_train, feed_dict={\n","                      self.tfs: bs, self.tfa: ba, self.tfr: br, self.tfs_: bs_})\n","        if self.learn_step_counter % 1000 == 0:     # delay training in order to stay curious\n","            self.sess.run(self.dyn_train, feed_dict={\n","                          self.tfs: bs, self.tfa: ba, self.tfs_: bs_})\n","        self.learn_step_counter += 1\n","\n","\n","env = gym.make('MountainCar-v0')\n","env = wrappers.Monitor(env, 'performance-1', force=True)\n","env._max_episode_steps = 50000\n","env = env.unwrapped\n","\n","dqn = CuriosityNet(n_a=3, n_s=2, lr=0.01, output_graph=False)\n","ep_steps = []\n","for epi in range(50):\n","    s = env.reset()\n","    steps = 0\n","    while True:\n","        # env.render()\n","        a = dqn.choose_action(s)\n","        s_, r, done, info = env.step(a)\n","        dqn.store_transition(s, a, r, s_)\n","        dqn.learn()\n","        if done:\n","            print('Epi: ', epi, \"| steps: \", steps)\n","            ep_steps.append(steps)\n","            break\n","        s = s_\n","        steps += 1\n","\n","plt.plot(ep_steps)\n","plt.ylabel(\"steps\")\n","plt.xlabel(\"episode\")\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZlgJS2PMUKFw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OAJNMh0lUKDj"},"source":[""],"execution_count":null,"outputs":[]}]}